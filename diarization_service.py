#!/usr/bin/env python3
"""
Flask —Å–µ—Ä–≤–∏—Å –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∞—É–¥–∏–æ
–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π –ø–æ Bearer token
"""

import os
import json
import tempfile
import torch
from flask import Flask, request, jsonify
from werkzeug.utils import secure_filename
from functools import wraps
from pyannote.audio import Pipeline

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
from dotenv import load_dotenv
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
def setup_proxy():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π"""
    http_proxy = os.getenv('HTTP_PROXY') or os.getenv('http_proxy')
    https_proxy = os.getenv('HTTPS_PROXY') or os.getenv('https_proxy')
    no_proxy = os.getenv('NO_PROXY') or os.getenv('no_proxy')
    
    proxies = {}
    
    if http_proxy or https_proxy:
        print("Proxy configuration detected:")
        if http_proxy:
            print(f"  HTTP:  {http_proxy}")
            os.environ['http_proxy'] = http_proxy
            os.environ['HTTP_PROXY'] = http_proxy
            proxies['http'] = http_proxy
        if https_proxy:
            print(f"  HTTPS: {https_proxy}")
            os.environ['https_proxy'] = https_proxy
            os.environ['HTTPS_PROXY'] = https_proxy
            proxies['https'] = https_proxy
        if no_proxy:
            os.environ['no_proxy'] = no_proxy
            os.environ['NO_PROXY'] = no_proxy
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è urllib
        try:
            import urllib.request
            proxy_handler = urllib.request.ProxyHandler(proxies)
            opener = urllib.request.build_opener(proxy_handler)
            urllib.request.install_opener(opener)
        except Exception as e:
            print(f"‚ö† Warning: Could not configure urllib proxy: {e}")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è huggingface_hub
        try:
            from huggingface_hub import configure_http_backend
            import requests
            
            session = requests.Session()
            session.proxies.update(proxies)
            session.verify = True
            
            configure_http_backend(backend_factory=lambda: session)
            print("‚úì HuggingFace Hub configured with proxy")
        except Exception as e:
            print(f"‚ö† Warning: Could not configure HF Hub proxy: {e}")
    
    return proxies

setup_proxy()

app = Flask(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BEARER_TOKEN = os.getenv('DIARIZATION_TOKEN', 'your-secret-token-here')
MODEL_PATH = os.getenv('PYANNOTE_MODEL_PATH', './models/pyannote-speaker-diarization-3.1')
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100 MB
ALLOWED_EXTENSIONS = {'mp3', 'wav', 'ogg', 'm4a', 'flac', 'wma'}

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
pipeline = None

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
def get_device():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∏–ª—É—á—à–µ–µ –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"üöÄ CUDA available: {gpu_name} ({gpu_memory:.1f} GB)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å
        torch.cuda.empty_cache()
        free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)
        free_memory_gb = free_memory / (1024**3)
        print(f"üíæ GPU memory available: {free_memory_gb:.1f} GB")
        
        if free_memory_gb < 2.0:
            print("‚ö†Ô∏è  Warning: Less than 2GB GPU memory available, performance may be limited")
        
        return device
    else:
        print("‚ö° CUDA not available, using CPU")
        return torch.device('cpu')

DEVICE = get_device()


def load_pipeline():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–∏—Å–∞"""
    global pipeline
    
    if pipeline is not None:
        return pipeline
    
    model_path = MODEL_PATH
    print(f"Loading diarization model...")
    print(f"Local path configured: {model_path}")
    
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏
        if model_path and os.path.exists(model_path):
            print(f"Loading from local path: {model_path}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∏ –∑–∞–º–µ–Ω—è–µ–º \ –Ω–∞ / –¥–ª—è pyannote
            abs_model_path = os.path.abspath(model_path).replace('\\', '/')
            print(f"Absolute path: {abs_model_path}")
            
            # –î–ª—è Windows –ø—É—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º file:// URI —Å—Ö–µ–º—É
            if os.name == 'nt':  # Windows
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞ –º–µ—Å—Ç–µ
                config_file = os.path.join(model_path, 'config.yaml')
                model_file = os.path.join(model_path, 'pytorch_model.bin')
                
                if not os.path.exists(config_file):
                    raise Exception(f"config.yaml not found in {model_path}")
                if not os.path.exists(model_file):
                    raise Exception(f"pytorch_model.bin not found in {model_path}")
                
                print("Found config.yaml and pytorch_model.bin")
                
            pipeline = Pipeline.from_pretrained(abs_model_path)
            pipeline = pipeline.to(DEVICE)
            print(f"‚úì Model loaded from local path and moved to {DEVICE}")
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ HuggingFace (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫—ç—à –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ —Å–∫–∞—á–∞–Ω–∞)
            hf_token = os.getenv('HF_TOKEN')
            
            print("Loading from HuggingFace (will use cache if available)...")
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
            if hf_token:
                os.environ['HF_TOKEN'] = hf_token
                print("Using HF_TOKEN for authentication")
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç—ã –¥–ª—è –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞/–ø—Ä–æ–∫—Å–∏
            os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600'
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–µ–∑ —è–≤–Ω–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ —Ç–æ–∫–µ–Ω–∞ - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∏–∑ env –∏–ª–∏ –∫—ç—à–∞
            pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1")
            pipeline = pipeline.to(DEVICE)
            print(f"‚úì Model loaded from HuggingFace cache and moved to {DEVICE}")
        
        return pipeline
    
    except Exception as e:
        print(f"‚úó Error loading model: {str(e)}")
        raise


def require_token(f):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ Bearer token"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        auth_header = request.headers.get('Authorization')
        
        if not auth_header:
            return jsonify({'error': 'No authorization header'}), 401
        
        try:
            scheme, token = auth_header.split()
            if scheme.lower() != 'bearer':
                return jsonify({'error': 'Invalid authorization scheme'}), 401
            
            if token != BEARER_TOKEN:
                return jsonify({'error': 'Invalid token'}), 401
        
        except ValueError:
            return jsonify({'error': 'Invalid authorization header format'}), 401
        
        return f(*args, **kwargs)
    
    return decorated_function


def allowed_file(filename):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–∞"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


@app.route('/health', methods=['GET'])
def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        model_status = "loaded" if pipeline is not None else "not loaded"
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ CUDA
        cuda_info = {
            'available': torch.cuda.is_available(),
            'current_device': str(DEVICE),
        }
        
        if torch.cuda.is_available():
            cuda_info.update({
                'device_count': torch.cuda.device_count(),
                'device_name': torch.cuda.get_device_name(0),
                'memory_total_gb': round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2),
                'memory_allocated_gb': round(torch.cuda.memory_allocated(0) / (1024**3), 2),
                'memory_cached_gb': round(torch.cuda.memory_reserved(0) / (1024**3), 2)
            })
        
        return jsonify({
            'status': 'healthy',
            'model': model_status,
            'device': cuda_info,
            'torch_version': torch.__version__
        }), 200
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e)
        }), 500


@app.route('/system', methods=['GET'])
def system_info():
    """–î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ –∏ CUDA"""
    try:
        system_info = {
            'python_version': f"{os.sys.version_info.major}.{os.sys.version_info.minor}.{os.sys.version_info.micro}",
            'torch_version': torch.__version__,
            'device': str(DEVICE),
            'cuda': {
                'available': torch.cuda.is_available(),
                'version': torch.version.cuda if torch.cuda.is_available() else None,
            }
        }
        
        if torch.cuda.is_available():
            system_info['cuda'].update({
                'device_count': torch.cuda.device_count(),
                'current_device': torch.cuda.current_device(),
                'devices': []
            })
            
            for i in range(torch.cuda.device_count()):
                device_props = torch.cuda.get_device_properties(i)
                device_info = {
                    'id': i,
                    'name': device_props.name,
                    'major': device_props.major,
                    'minor': device_props.minor,
                    'total_memory_gb': round(device_props.total_memory / (1024**3), 2),
                    'multiprocessor_count': device_props.multi_processor_count
                }
                
                # –¢–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –ø–æ–ª—É—á–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
                if i == torch.cuda.current_device():
                    device_info.update({
                        'memory_allocated_gb': round(torch.cuda.memory_allocated(i) / (1024**3), 2),
                        'memory_cached_gb': round(torch.cuda.memory_reserved(i) / (1024**3), 2),
                        'memory_free_gb': round((device_props.total_memory - torch.cuda.memory_reserved(i)) / (1024**3), 2)
                    })
                
                system_info['cuda']['devices'].append(device_info)
        
        return jsonify(system_info), 200
        
    except Exception as e:
        return jsonify({
            'error': str(e)
        }), 500


@app.route('/diarize', methods=['POST'])
@require_token
def diarize():
    """
    –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∞—É–¥–∏–æ
    
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç:
    - file: –∞—É–¥–∏–æ—Ñ–∞–π–ª (multipart/form-data)
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    - JSON —Å –º–∞—Å—Å–∏–≤–æ–º —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (speaker, start, end)
    """
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400
    
    file = request.files['file']
    
    if file.filename == '':
        return jsonify({'error': 'Empty filename'}), 400
    
    if not allowed_file(file.filename):
        return jsonify({'error': f'File type not allowed. Allowed: {", ".join(ALLOWED_EXTENSIONS)}'}), 400
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
    file.seek(0, os.SEEK_END)
    file_size = file.tell()
    file.seek(0)
    
    if file_size > MAX_FILE_SIZE:
        return jsonify({'error': f'File too large. Max size: {MAX_FILE_SIZE / 1024 / 1024} MB'}), 400
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    temp_file = None
    try:
        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp:
            temp_file = tmp.name
            file.save(tmp.name)
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
        import time
        print(f"üéµ Processing file: {file.filename} ({file_size / 1024:.2f} KB) on {DEVICE}")
        
        # –û—á–∏—â–∞–µ–º GPU –∫—ç—à –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        start_time = time.time()
        model = load_pipeline()
        
        # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
        diarization_start = time.time()
        diarization = model(temp_file)
        diarization_time = time.time() - diarization_start
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            result.append({
                "start": float(turn.start),
                "end": float(turn.end),
                "speaker": speaker
            })
        
        total_time = time.time() - start_time
        print(f"‚úì Diarization completed: {len(result)} segments in {diarization_time:.2f}s (total: {total_time:.2f}s)")
        
        return jsonify({
            'success': True,
            'segments': result,
            'total_segments': len(result),
            'processing_time_seconds': round(diarization_time, 2),
            'total_time_seconds': round(total_time, 2),
            'device': str(DEVICE)
        }), 200
    
    except Exception as e:
        print(f"‚úó Error during diarization: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500
    
    finally:
        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        if temp_file and os.path.exists(temp_file):
            try:
                os.unlink(temp_file)
            except:
                pass


if __name__ == '__main__':
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    try:
        load_pipeline()
    except Exception as e:
        print(f"WARNING: Could not load model at startup: {str(e)}")
        print("Model will be loaded on first request")
    
    # –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
    port = int(os.getenv('DIARIZATION_PORT', 5000))
    host = os.getenv('DIARIZATION_HOST', '0.0.0.0')
    
    print(f"\n{'='*60}")
    print(f"Diarization Service Starting")
    print(f"{'='*60}")
    print(f"Host: {host}")
    print(f"Port: {port}")
    print(f"Token: {BEARER_TOKEN[:10]}... (set via DIARIZATION_TOKEN)")
    print(f"Model: {MODEL_PATH}")
    print(f"{'='*60}\n")
    
    app.run(host=host, port=port, debug=False)
